{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4qr56wghDp"
      },
      "source": [
        "# **GPT Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zKUWRW2gqsg",
        "outputId": "cf83bff0-7d09-4c66-ae7f-b871e7efa357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (3.13.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2025.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28) (4.15.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.9.0\n",
            "    Uninstalling openai-2.9.0:\n",
            "      Successfully uninstalled openai-2.9.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pLybHbO1gwM6"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFubT9CsisU7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "openai.api_key = \"sk-................\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbwYX_ytg6d7"
      },
      "source": [
        "# **1. Ground Truth Grammars**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLnGeuvskift"
      },
      "source": [
        "Grammars for SYN, SYNACK and ACK packets (json files) are in ground_truth_grammar folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEw05-7Ikn1k"
      },
      "source": [
        "# **2. Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fJgBhpdo707",
        "outputId": "ace065b6-2fc1-44c8-c7b3-ecdc11a656c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Processing PacketType=SYN ###\n",
            "  -> LLM=gpt-4.1 | Prompting=baseline\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-4.1 | Prompting=oneshot_cot\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-3.5-turbo | Prompting=baseline\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-3.5-turbo | Prompting=oneshot_cot\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "\n",
            "### Processing PacketType=SYN-ACK ###\n",
            "  -> LLM=gpt-4.1 | Prompting=baseline\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-4.1 | Prompting=oneshot_cot\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-3.5-turbo | Prompting=baseline\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-3.5-turbo | Prompting=oneshot_cot\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "\n",
            "### Processing PacketType=ACK ###\n",
            "  -> LLM=gpt-4.1 | Prompting=baseline\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-4.1 | Prompting=oneshot_cot\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-3.5-turbo | Prompting=baseline\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "  -> LLM=gpt-3.5-turbo | Prompting=oneshot_cot\n",
            "     Run 1/20\n",
            "     Run 2/20\n",
            "     Run 3/20\n",
            "     Run 4/20\n",
            "     Run 5/20\n",
            "     Run 6/20\n",
            "     Run 7/20\n",
            "     Run 8/20\n",
            "     Run 9/20\n",
            "     Run 10/20\n",
            "     Run 11/20\n",
            "     Run 12/20\n",
            "     Run 13/20\n",
            "     Run 14/20\n",
            "     Run 15/20\n",
            "     Run 16/20\n",
            "     Run 17/20\n",
            "     Run 18/20\n",
            "     Run 19/20\n",
            "     Run 20/20\n",
            "\n",
            "✔ Saved tcp_task1a_runs_llm_prompting.csv\n",
            "✔ Saved tcp_task1a_summary_llm_prompting.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# ==========================\n",
        "# CONFIG\n",
        "# ==========================\n",
        "\n",
        "# Set your key safely (recommended):\n",
        "# In Colab: os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
        "openai.api_key = \"sk-................\"\n",
        "\n",
        "MODELS = [\"gpt-4.1\", \"gpt-3.5-turbo\"]\n",
        "PROMPTING_MODES = [\"baseline\", \"oneshot_cot\"]\n",
        "N_REPETITIONS = 20\n",
        "\n",
        "# Ground truth grammar files (your paths)\n",
        "GROUND_TRUTH_FILES = {\n",
        "    \"SYN\":     \"ground_truth_grammar/SYN.json\",\n",
        "    \"SYN-ACK\": \"ground_truth_grammar/SYNACK.json\",\n",
        "    \"ACK\":     \"ground_truth_grammar/ACK.json\",\n",
        "}\n",
        "\n",
        "# One-shot example: use SYN grammar (same as your uploaded SYN) :contentReference[oaicite:1]{index=1}\n",
        "ONE_SHOT_SYN_PATH = \"ground_truth_grammar/SYN.json\"\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# HELPERS\n",
        "# ==========================\n",
        "\n",
        "def normalize_quotes(text: str) -> str:\n",
        "    return (\n",
        "        text.replace(\"“\", \"\\\"\")\n",
        "            .replace(\"”\", \"\\\"\")\n",
        "            .replace(\"‘\", \"'\")\n",
        "            .replace(\"’\", \"'\")\n",
        "    )\n",
        "\n",
        "def load_text_file(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return normalize_quotes(f.read())\n",
        "\n",
        "def load_gt_file(path: str):\n",
        "    \"\"\"\n",
        "    Reads a JSON-like grammar file and extracts:\n",
        "      - label (e.g., TCP_SYN)\n",
        "      - dict of fields -> values\n",
        "      - raw text (for prompt formatting)\n",
        "    \"\"\"\n",
        "    raw = load_text_file(path)\n",
        "\n",
        "    label_match = re.search(r'\"([^\"]+)\"\\s*:\\s*\\{', raw)\n",
        "    if not label_match:\n",
        "        raise ValueError(f\"Could not find packet label in {path}\")\n",
        "    label = label_match.group(1)\n",
        "\n",
        "    body_match = re.search(r'\\{(.*)\\}', raw, flags=re.DOTALL)\n",
        "    body = body_match.group(1) if body_match else \"\"\n",
        "\n",
        "    pairs = re.findall(r'\"([^\"]+)\"\\s*:\\s*\"([^\"]*)\"', body)\n",
        "    fields = {k.strip(): v.strip() for k, v in pairs}\n",
        "    return label, fields, raw\n",
        "\n",
        "def parse_grammar(text: str):\n",
        "    \"\"\"\n",
        "    Extracts key/value pairs from the model output object.\n",
        "    \"\"\"\n",
        "    text = normalize_quotes(text)\n",
        "    body_match = re.search(r'\\{(.*)\\}', text, flags=re.DOTALL)\n",
        "    if not body_match:\n",
        "        return {}\n",
        "    body = body_match.group(1)\n",
        "\n",
        "    pairs = re.findall(r'\"([^\"]+)\"\\s*:\\s*\"([^\"]*)\"', body)\n",
        "    return {k.strip(): v.strip() for k, v in pairs}\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# ONE-SHOT EXAMPLE (SYN)\n",
        "# ==========================\n",
        "\n",
        "SYN_ONE_SHOT_TEXT = load_text_file(ONE_SHOT_SYN_PATH)  # uses SYN grammar verbatim\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# PROMPT BUILDERS\n",
        "# ==========================\n",
        "\n",
        "def build_prompt_baseline(pkt_type: str, gt_label: str, gt_text: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a protocol grammar extraction assistant.\n",
        "\n",
        "Reconstruct the TCP grammar for:\n",
        "\n",
        "    \"{gt_label}\"\n",
        "\n",
        "Follow EXACTLY the same formatting style shown here:\n",
        "\n",
        "{gt_text}\n",
        "\n",
        "Rules:\n",
        "- Use EXACT same field names.\n",
        "- DO NOT add or remove fields.\n",
        "- Use ONLY double quotes around names and values.\n",
        "- Output only the {gt_label} grammar object.\n",
        "- No explanations outside the grammar.\n",
        "\"\"\".strip()\n",
        "\n",
        "def build_prompt_oneshot_cot(pkt_type: str, gt_label: str, gt_text: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a protocol grammar extraction assistant.\n",
        "\n",
        "Below is ONE example of a TCP grammar (for a SYN packet).\n",
        "Use it ONLY to learn the formatting and structure.\n",
        "\n",
        "ONE-SHOT EXAMPLE:\n",
        "{SYN_ONE_SHOT_TEXT}\n",
        "\n",
        "Now reconstruct the TCP grammar for:\n",
        "\n",
        "\"{gt_label}\"\n",
        "\n",
        "You must follow EXACTLY the same formatting style as the example.\n",
        "\n",
        "Reference format (field names and ordering must match):\n",
        "{gt_text}\n",
        "\n",
        "Reasoning:\n",
        "Think step-by-step internally to ensure correctness.\n",
        "Do NOT reveal your reasoning.\n",
        "\n",
        "Rules:\n",
        "- Use EXACT same field names as the reference.\n",
        "- DO NOT add or remove fields.\n",
        "- Use ONLY double quotes around names and values.\n",
        "- Output ONLY the \"{gt_label}\" grammar object.\n",
        "- No explanations or extra text.\n",
        "\"\"\".strip()\n",
        "\n",
        "def build_prompt(pkt_type: str, gt_label: str, gt_text: str, prompting_mode: str) -> str:\n",
        "    if prompting_mode == \"baseline\":\n",
        "        return build_prompt_baseline(pkt_type, gt_label, gt_text)\n",
        "    elif prompting_mode == \"oneshot_cot\":\n",
        "        return build_prompt_oneshot_cot(pkt_type, gt_label, gt_text)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prompting_mode: {prompting_mode}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# GPT CALL\n",
        "# ==========================\n",
        "\n",
        "def call_gpt(prompt: str, model_name: str) -> str:\n",
        "    # temperature>0 ensures variation across runs\n",
        "    resp = openai.ChatCompletion.create(\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful protocol analysis assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return resp.choices[0].message[\"content\"]\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# EVALUATION\n",
        "# ==========================\n",
        "\n",
        "def evaluate(gt_fields, model_fields, pkt_type):\n",
        "    field_names = list(gt_fields.keys())\n",
        "\n",
        "    missing_fields = 0\n",
        "    correct_positions = 0\n",
        "    missing_value_pairs = 0\n",
        "\n",
        "    for f in field_names:\n",
        "        if f not in model_fields or model_fields[f] == \"\":\n",
        "            missing_fields += 1\n",
        "            missing_value_pairs += 1\n",
        "        else:\n",
        "            if model_fields[f] == gt_fields[f]:\n",
        "                correct_positions += 1\n",
        "            else:\n",
        "                missing_value_pairs += 1\n",
        "\n",
        "    total_fields = len(field_names)\n",
        "\n",
        "    return {\n",
        "        \"Packet Type\": pkt_type,\n",
        "        \"Number of Packet Fields (Ground Truth)\": total_fields,\n",
        "        \"Number of Fields Extracted\": len(model_fields),\n",
        "\n",
        "        # Kept your original metric names for compatibility\n",
        "        \"Average Number of Missing Fields\": missing_fields / 1,\n",
        "        \"Average Number of Correct Field Positions\": correct_positions / total_fields if total_fields else 0.0,\n",
        "        \"Average Number of Missing Field Value Pairs\": missing_value_pairs / total_fields if total_fields else 0.0,\n",
        "    }\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# MAIN PIPELINE\n",
        "# ==========================\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for pkt_type, path in GROUND_TRUTH_FILES.items():\n",
        "    print(f\"\\n### Processing PacketType={pkt_type} ###\")\n",
        "\n",
        "    gt_label, gt_fields, gt_text = load_gt_file(path)\n",
        "\n",
        "    for llm in MODELS:\n",
        "        for prompting_mode in PROMPTING_MODES:\n",
        "            print(f\"  -> LLM={llm} | Prompting={prompting_mode}\")\n",
        "\n",
        "            for i in range(N_REPETITIONS):\n",
        "                print(f\"     Run {i+1}/{N_REPETITIONS}\")\n",
        "\n",
        "                prompt = build_prompt(pkt_type, gt_label, gt_text, prompting_mode)\n",
        "                output = call_gpt(prompt, llm)\n",
        "\n",
        "                model_fields = parse_grammar(output)\n",
        "                metrics = evaluate(gt_fields, model_fields, pkt_type)\n",
        "\n",
        "                # identifiers for combined CSV\n",
        "                metrics[\"LLM\"] = llm\n",
        "                metrics[\"Prompting\"] = prompting_mode\n",
        "                metrics[\"Iteration\"] = i + 1\n",
        "                metrics[\"RawOutput\"] = output  # optional but useful\n",
        "\n",
        "                all_runs.append(metrics)\n",
        "\n",
        "                time.sleep(0.2)\n",
        "\n",
        "# ==========================\n",
        "# SAVE RESULTS\n",
        "# ==========================\n",
        "\n",
        "df_all = pd.DataFrame(all_runs)\n",
        "\n",
        "# Reorder columns nicely\n",
        "front_cols = [\"LLM\", \"Prompting\", \"Packet Type\", \"Iteration\"]\n",
        "other_cols = [c for c in df_all.columns if c not in front_cols]\n",
        "df_all = df_all[front_cols + other_cols]\n",
        "\n",
        "df_all.to_csv(\"tcp_task1a_runs_llm_prompting.csv\", index=False)\n",
        "print(\"\\n✔ Saved tcp_task1a_runs_llm_prompting.csv\")\n",
        "\n",
        "# Optional: summary CSV (mean/std grouped by LLM + Prompting + Packet Type)\n",
        "df_summary = (\n",
        "    df_all.groupby([\"LLM\", \"Prompting\", \"Packet Type\"], as_index=False)\n",
        "         .agg({\n",
        "             \"Average Number of Missing Fields\": [\"mean\", \"std\"],\n",
        "             \"Average Number of Correct Field Positions\": [\"mean\", \"std\"],\n",
        "             \"Average Number of Missing Field Value Pairs\": [\"mean\", \"std\"],\n",
        "         })\n",
        ")\n",
        "\n",
        "# Flatten multi-index columns\n",
        "df_summary.columns = [\n",
        "    \"_\".join([c for c in col if c]).replace(\"__\", \"_\")\n",
        "    for col in df_summary.columns.values\n",
        "]\n",
        "\n",
        "df_summary.to_csv(\"tcp_task1a_summary_llm_prompting.csv\", index=False)\n",
        "print(\"✔ Saved tcp_task1a_summary_llm_prompting.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPOV4H31rRP2"
      },
      "source": [
        "# **3. FInal Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01zDzuPbrVzo",
        "outputId": "4c8ab86b-084b-47fb-8515-add4f71e2e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ Saved tcp_task1a_summary_llm_prompting.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df_summary.to_csv(\"tcp_task1a_summary_llm_prompting.csv\", index=False)\n",
        "print(\"✔ Saved tcp_task1a_summary_llm_prompting.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
